{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Bipedalwalker-V3_TD3_Tensorboard.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hyyN-2qyK_T2",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "# **OpenAI Gym BiPedalwalker-V3 TD3 with Tensorboard & Video** \n",
        "\n",
        "This colab will allow you to train, evaluate and visulize your results using stable-baselines and tensorboard. Google colab don't support env.render() so we will use a work around where we \"fake\" a display, record a video and then display it. We will be using OpenAI Gym enviorment,  Stable-baselines & TD3\n",
        "\n",
        "## **Instructions**\n",
        "Click **Open in playground** top left corner.   \n",
        "Then either run cell by cell (recommended)   \n",
        "or just click \"Runtime\" in toolbar, then \"Run all\" leave the tab running,  \n",
        "check back in 30-60 min and scroll down top bottom\n",
        "\n",
        "### **Links**\n",
        "[https://github.com/openai/gym/wiki/BipedalWalker-v2](https://github.com/openai/gym/wiki/BipedalWalker-v2)  \n",
        "[https://stable-baselines.readthedocs.io/en/master/](https://stable-baselines.readthedocs.io/en/master/)  \n",
        "[https://towardsdatascience.com/td3-learning-to-run-with-ai-40dfc512f93](https://towardsdatascience.com/td3-learning-to-run-with-ai-40dfc512f93)\n",
        "\n",
        "----\n",
        "\n",
        "# **A Notebook from Nextgrid.ai**\n",
        "![Nextgrid Deep learning labs](https://nextgrid.ai/wp-content/uploads/2020/01/deep-learning-labs-scaled.jpg)\n",
        "\n",
        " \n",
        "### **Nextgrid** - _The **Superlative** destination for deep & reinforcement learning startups & talent_\n",
        "\n",
        "Learn more: [Deep learning labs](https://nextgrid.ai/deep-learning-labs/) / [Nextgrid](https://nextgrid.ai) \n",
        "\n",
        "\n",
        "\n",
        "▪️️️️️️️▪️️️️️️️▪️️️️️️️▪️️️️️️️▪️️️️️️️▪️️️️️️️▪️️️️️️️▪️️️️️️️  \n",
        "*Notebook by Mathias*  \n",
        "*I would love your feedback,*  \n",
        "*or discuss your DL/DRL startup/business idea.*   \n",
        "*find me on* _[twitter](https://twitter.com/mathiiias123)_ or _[linkedin](https://www.linkedin.com/in/imathias)_\n",
        "\n",
        "\n",
        "\n",
        "#### **Changelog** \n",
        "```\n",
        "2020/02/09 - Updated package versions and switched to Bipedalwalker-V3\n",
        "2020/04/08 - Updated to stable-baselines 2.10.0 & Tensorboard issue workaround\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22TBe2qeFlyr",
        "colab_type": "text"
      },
      "source": [
        "## Install system wide packages\n",
        "Install linux server packages using `apt-get` and Python packages using `pip`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gWskDE2c9WoN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!sudo apt-get update\n",
        "!apt-get install swig cmake python3-dev libopenmpi-dev zlib1g-dev xvfb x11-utils ffmpeg #remove -qq for full output\n",
        "\n",
        "%tensorflow_version 1.x\n",
        "%load_ext tensorboard\n",
        "\n",
        "!pip install stable-baselines==2.10.0 box2d box2d-kengz pyvirtualdisplay pyglet==1.5.0 --quiet #remove --quiet for full output "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FtY8FhliLsGm",
        "colab_type": "text"
      },
      "source": [
        "## Dependencis\n",
        "import dependencis required to run, train & record video"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2pToLfvOzCKQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gym\n",
        "import imageio\n",
        "import time\n",
        "import numpy as np\n",
        "import base64\n",
        "import IPython\n",
        "import PIL.Image\n",
        "import pyvirtualdisplay\n",
        "\n",
        "\n",
        "# Video \n",
        "from pathlib import Path\n",
        "from IPython import display as ipythondisplay\n",
        "\n",
        "# Stable baselines\n",
        "from stable_baselines import TD3\n",
        "from stable_baselines.td3.policies import MlpPolicy\n",
        "from stable_baselines.common.vec_env import DummyVecEnv\n",
        "from stable_baselines.ddpg.noise import NormalActionNoise, OrnsteinUhlenbeckActionNoise\n",
        "from stable_baselines.common.vec_env import VecVideoRecorder, SubprocVecEnv, DummyVecEnv\n",
        "from stable_baselines.common.evaluation import evaluate_policy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WlaTq_UvGKo-",
        "colab_type": "text"
      },
      "source": [
        "# Define & Configure our Reinforcment learning algo\n",
        "Here we define our variables & Hyperparamters  \n",
        "In this example we are using default Twin Delayed DDPG.  \n",
        "Read more about how you define your TD3 [parameters](https://stable-baselines.readthedocs.io/en/master/modules/td3.html#parameters) "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tKA52SBe6JdJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Variables\n",
        "env_id = 'BipedalWalker-v3'\n",
        "video_folder = '/videos'\n",
        "video_length = 3000\n",
        "logs_base_dir = './runs' # Log DIR\n",
        "steps_total= 0 # Keep track of total steps\n",
        "\n",
        "\n",
        "### Enviorment \n",
        "env = DummyVecEnv([lambda: gym.make(env_id)])\n",
        "obs = env.reset()\n",
        "score = 0\n",
        "log_interval = 10          # Print avg reward after interval\n",
        "\n",
        "\n",
        "### Hyperparameters \n",
        "\n",
        "# Action noise\n",
        "n_actions = env.action_space.shape[-1]\n",
        "action_noise = NormalActionNoise(mean=np.zeros(n_actions), sigma=0.1 * np.ones(n_actions))\n",
        "\n",
        "# Model configuration\n",
        "model = TD3(\n",
        "    MlpPolicy,\n",
        "    env,\n",
        "    verbose=1,                         # display output when training, 0 = no output, 1 = show output\n",
        "    gamma=0.99,                        # discount for future rewards\n",
        "    learning_rate=0.003,               # learning rate\n",
        "    buffer_size=100000,                # size of the replay buffer\n",
        "    batch_size=1000,                    # number of transitions sampled from replay buffer\n",
        "    learning_starts=500,                # steps before starting training\n",
        "    train_freq=1000,                   # update the model every train_freq steps.\n",
        "    gradient_steps=1000,               # how many gradient update after each step\n",
        "    #tau=0.005,                        # the soft update coefficient (“polyak update” of the target networks, between 0 and 1)\n",
        "    #policy_delay=2,                   # policy and target networks will only be updated once every policy_delay steps per training steps. The Q values will be updated policy_delay more often (update every training step).\n",
        "    #action_noise=action_noise,        # action noise type. Cf DDPG for the different action noise type.\n",
        "    #target_policy_noise=0.2,          # standard deviation of Gaussian noise added to target policy \n",
        "    #target_noise_clip=0.5,            # limit for absolute value of target policy smoothing noise.\n",
        "    #random_exploration=0.0,           # probability of taking a random action\n",
        "    n_cpu_tf_sess=None,                # number of threads for TensorFlow operations If None, the number of cpu of the current machine will be used.\n",
        "\n",
        "    # Tensorboard stuff\n",
        "    tensorboard_log=logs_base_dir,\n",
        "    full_tensorboard_log=True, \n",
        "\n",
        "    # seed=None, \n",
        "    # _init_setup_model=True, \n",
        "    # \n",
        "    )\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pmK85N-4XdK7",
        "colab_type": "text"
      },
      "source": [
        "## Training & Rec/Play Video [functions]\n",
        "\n",
        "- `def learning(name, steps=10000, prefix=env_id, eval=1000):`\n",
        "- `def record(name, length=1500):`  \n",
        "\n",
        "_that simply help us call the right functions to train our agent and to record & display video_ "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BIedd7Pz9sOs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Training function\n",
        "def learning(name, steps=10000, prefix=env_id, eval=1000):\n",
        "  model.learn(total_timesteps=steps, log_interval=log_interval)\n",
        "  model.save(name + \"-\" + prefix)\n",
        "  # Random Agent, after training\n",
        "  # mean_reward_after_train = evaluate(model, num_steps=eval)\n",
        "\n",
        "\n",
        "def record(name, length=1500):\n",
        "   record_video(env_id, model, video_length=length, prefix=name)\n",
        "   show_videos('videos', prefix=name)\n",
        "   print(name, \" steps total\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6qcuGpFzhIZY",
        "colab_type": "text"
      },
      "source": [
        "## Functions\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H-qEyqYl86uI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Record & Display Video\n",
        "\n",
        "import os\n",
        "os.system(\"Xvfb :1 -screen 0 1024x768x24 &\")\n",
        "os.environ['DISPLAY'] = ':1'\n",
        "\n",
        "# Record video\n",
        "def record_video(env_id, model, video_length=500, prefix='', video_folder='videos/'):\n",
        "  \"\"\"\n",
        "  :param env_id: (str)\n",
        "  :param model: (RL model)\n",
        "  :param video_length: (int)\n",
        "  :param prefix: (str)\n",
        "  :param video_folder: (str)\n",
        "  \"\"\"\n",
        "  eval_env = DummyVecEnv([lambda: gym.make(env_id)])\n",
        "  # Start the video at step=0 and record 500 steps\n",
        "  eval_env = VecVideoRecorder(env, video_folder=video_folder,\n",
        "                              record_video_trigger=lambda step: step == 0, video_length=video_length,\n",
        "                              name_prefix=prefix)\n",
        "\n",
        "  obs = eval_env.reset()\n",
        "  for _ in range(video_length):\n",
        "    action, _ = model.predict(obs)\n",
        "    obs, _, _, _ = eval_env.step(action)\n",
        "\n",
        "  # Close the video recorder\n",
        "  eval_env.close()\n",
        "\n",
        "\n",
        "## Display video\n",
        "def show_videos(video_path='', prefix=''):\n",
        "  html = []\n",
        "  for mp4 in Path(video_path).glob(\"{}*.mp4\".format(prefix)):\n",
        "      video_b64 = base64.b64encode(mp4.read_bytes())\n",
        "      html.append('''<video alt=\"{}\" autoplay \n",
        "                    loop controls style=\"height: 400px;\">\n",
        "                    <source src=\"data:video/mp4;base64,{}\" type=\"video/mp4\" />\n",
        "                </video>'''.format(mp4, video_b64.decode('ascii')))\n",
        "  ipythondisplay.display(ipythondisplay.HTML(data=\"<br>\".join(html)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YJQe53BM0ARa",
        "colab_type": "text"
      },
      "source": [
        "# Display Tensorboard inline\n",
        "Run & Display tensorboard   \n",
        "**PS.** *sometimes it does not show up at all, then test to uncomment the reload code, or jusrt run cell again*\n",
        "\n",
        "It's correctly loaded when you see this view\n",
        "![Tensorboard](https://nextgrid.ai/wp-content/uploads/2019/12/Screenshot-2019-12-27-at-16.40.02.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TK5g4Xbc6HMs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Often not loading on first try, run again until u see the screen\n",
        "%tensorboard --logdir {logs_base_dir}\n",
        "%reload_ext tensorboard"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9PP-0aa90TNz",
        "colab_type": "text"
      },
      "source": [
        "# Training Function\n",
        "We want to automate the training function so that it will keep running until the result we looking for is achived"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9cQ_Nt7d1NOP",
        "colab": {}
      },
      "source": [
        "\n",
        "def run_training(steps_per_round=200000,limit=300):\n",
        "# This function will run a training with value set in `steps_per_round`\n",
        "# after each round it will messure it's value, If value is under `limit` it will keep training until score limit is reached.  \n",
        "\n",
        "  global score\n",
        "  global steps_total\n",
        "\n",
        "  print(\"Training is starting.. \")\n",
        "  \n",
        "  while score < limit:\n",
        "      steps_total = steps_total + steps_per_round\n",
        "      learning(str(steps_total), steps=steps_per_round)\n",
        "      new_evaluation = evaluate_policy(model, env, n_eval_episodes=10, deterministic=True, render=False, callback=None, reward_threshold=None, return_episode_rewards=False)\n",
        "      score = new_evaluation[0]\n",
        "      record(name=steps_total, length=1000) # uncomment to show video from each round\n",
        "      print(\"Mean reward:\", score )\n",
        "    \n",
        "\n",
        "  # Threshold reached > evaluate over 100 episodes > Video rec/display\n",
        "  print(\"Reward limit achived, messuring over 100ep & recording video, please wait...\")\n",
        "  record(name=steps_total, length=1750)\n",
        "  ep100 = evaluate_policy(model, env, n_eval_episodes=50, deterministic=True, render=False, callback=None, reward_threshold=None, return_episode_rewards=True)\n",
        "  print(\"Mean Reward 100 Epispodes: \", ep100[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iJbbuMF60QmB",
        "colab_type": "text"
      },
      "source": [
        "## Train moodel\n",
        "Add the amount of total moves that will be run before messuring results with `steps_per_round` parameter.  In `limnit` add the score you want model to reach to end training. If not reached it will simply run another round."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i-imjiLk0Odz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Traing\n",
        "run_training(steps_per_round=100000,limit=100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ecisEsrX0dKu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "run_training(steps_per_round=100000,limit=200)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "odLPx9ogu31Z",
        "colab_type": "text"
      },
      "source": [
        "# Evaluation\n",
        "OpenAI scores is generally messured over 100 epochs. Use code belowe to messure your avarage score over 100 rounds"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U7D2_Fjzf935",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "evals = evaluate_policy(model, env, n_eval_episodes=100, deterministic=True, render=False, callback=None, reward_threshold=None, return_episode_rewards=False)\n",
        "print(evals)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gMR_hsEhk_bw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Code demostrating how to save, delete & load model\n",
        "# model.save(\"save_as_name\")\n",
        "# del model # \n",
        "# model = TD3.load(\"name_of_model_to_load\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nfk5JmejMDoB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yFGsuEeAfUBY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VZAVOQYwTeAF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}